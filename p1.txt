import torch
import torch.nn as nn
import torch.optim as optim

print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

corpus = "we are learning deep learning using neural networks".split()
vocab = set(corpus)
word2idx = {word: i for i, word in enumerate(vocab)}
idx2word = {i: word for word, i in word2idx.items()}

def generate_pairs(corpus, window_size=2):
    pairs = []
    for i, target in enumerate(corpus):
        for j in range(-window_size, window_size + 1):
            if j != 0 and 0 <= i + j < len(corpus):
                pairs.append((target, corpus[i + j]))
    return pairs

pairs = generate_pairs(corpus)

class Word2Vec(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.linear = nn.Linear(embed_dim, vocab_size)
    def forward(self, x):
        embed = self.embedding(x)
        return self.linear(embed)

model = Word2Vec(len(vocab), 10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

for epoch in range(100):
    loss = 0
    for target, context in pairs:
        input_tensor = torch.tensor([word2idx[target]])
        label = torch.tensor([word2idx[context]])
        output = model(input_tensor)
        loss_val = criterion(output, label)
        optimizer.zero_grad()
        loss_val.backward()
        optimizer.step()
        loss += loss_val.item()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")
